{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE pretrained network to output keypoint's description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64\n",
    "from munkres import Munkres\n",
    "from descriptor_CNN3 import DesNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pre-trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DesNet()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "weight_path = \"checkpoint.pth\"\n",
    "trained_weight = torch.load(weight_path)\n",
    "model.load_state_dict(trained_weight['state_dict'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 1, 32, 32])\n",
      "torch.Size([35, 30, 1, 32, 32])\n",
      "torch.Size([175, 30, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "patches_dir_images = \"../keypoint_detector/patches_images.pt\"\n",
    "patches_dir_query = \"../keypoint_detector/patches_query.pt\"\n",
    "patches_dir_all = \"../keypoint_detector/patches_all.pt\"\n",
    "patches_images = torch.load(patches_dir_images)\n",
    "patches_query = torch.load(patches_dir_query)\n",
    "patches_all = torch.load(patches_dir_all)\n",
    "\n",
    "print(patches_images.shape)\n",
    "print(patches_query.shape)\n",
    "print(patches_all.shape)\n",
    "\n",
    "patches_query =  patches_query.view(-1, 1, 32, 32).cuda()\n",
    "patches_images =  patches_images.view(-1, 1, 32, 32).cuda()\n",
    "patches_all =  patches_all.view(-1, 1, 32, 32).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and save deep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 30, 128])\n",
      "torch.Size([35, 30, 128])\n",
      "torch.Size([175, 30, 128])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    description_images = model(patches_images)\n",
    "    description_images = description_images.view(-1, 30, 128).cpu().data\n",
    "    description_query = model(patches_query)\n",
    "    description_query = description_query.view(-1, 30, 128).cpu().data\n",
    "    description_all = model(patches_all)\n",
    "    description_all = description_all.view(-1, 30, 128).cpu().data\n",
    "\n",
    "    print(description_images.shape)\n",
    "    print(description_query.shape)\n",
    "    print(description_all.shape)\n",
    "    \n",
    "## Save deep features  \n",
    "# IMAGES\n",
    "output_dir_images = \"images_keypoints_descriptions.pt\"\n",
    "torch.save(description_images, output_dir_images)\n",
    "\n",
    "# QUERY\n",
    "output_dir_query = \"query_keypoints_descriptions.pt\"\n",
    "torch.save(description_query, output_dir_query)\n",
    "\n",
    "# QUERY + IMAGES\n",
    "output_dir_query_and_images = \"query_and_images_keypoints_descriptions.pt\"\n",
    "torch.save(description_all, output_dir_query_and_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Similitude Matrix - One to One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.09582365 15.95460675 15.27731852 10.70013843  9.43909224  9.34128532\n",
      "  9.05646155  9.20370808  9.2096375   9.03209764  9.13348832  9.26411\n",
      "  9.46661394  9.51117939  9.44195185  9.50794791  9.96176838  9.55204065\n",
      "  9.58444671  9.92530709  9.13752038  9.46949094  9.50596298  9.25694003\n",
      "  9.13965751  8.69106763  9.11704305  9.05366091  9.02702301  8.78656244\n",
      "  8.98598215  8.93882482  9.37711506  8.99058854  9.12011687  9.17869833\n",
      "  8.81769334  9.02213751  8.88353731  8.89595945  8.76698522  8.57958937\n",
      "  8.74478663  8.85770288  8.82254653  8.81951981  9.09401025  8.88411739\n",
      "  9.15835969  8.820418    8.93806016  9.29447639  9.16253198  9.00225674\n",
      "  8.96339852  9.14523837  8.83033947  8.93134173  8.81629518  8.75699357\n",
      "  9.24026304  9.12726467  9.15305131  8.92957916  8.36546713  8.58626647\n",
      "  8.51551737  8.45373247  8.67202614  8.5763631   8.48536445  8.96527843\n",
      "  9.07761565  9.06382175  9.14821979  9.15286827  9.17176009  9.0462184\n",
      "  8.94315183  8.86413765  9.0145628   8.97380707  9.03483239  8.9288328\n",
      "  8.8090857   8.81749544  9.09763368  8.91977129  8.96627571  8.74504793\n",
      "  8.9176043   8.95431828  8.58253263  8.8504372   9.40196991  9.46670561\n",
      "  8.46928032  8.34499512  8.49295448  8.49760836  8.51589998  8.44399171\n",
      "  8.40550817  8.47754281  8.73779462  8.39607906  8.30552996  8.57135058\n",
      "  8.94845789  8.79658522  9.09110888  8.67572368  9.24355266  9.07084749\n",
      "  8.91132801  8.87218422  9.37136856  8.83620054  8.89882597  9.136086\n",
      "  9.00310392  8.93368921  9.28437314  9.41690865  9.45206325  8.94777875\n",
      "  9.10542623  9.33331395  8.89681667  9.05979125  9.01267979  9.15902776\n",
      "  8.91056329  8.93204367  8.82803428  9.03542034  9.02800398  8.89354472\n",
      "  8.96293825  8.82521085]\n"
     ]
    }
   ],
   "source": [
    "from munkres import Munkres\n",
    "\n",
    "one_to_one_similitude_matrix = np.zeros((35,140))\n",
    "hungarian = Munkres()\n",
    "\n",
    "for qkeypoint in range(0,35):\n",
    "    for images_keypoint in range(0,140):\n",
    "        norm = np.zeros((30,30))\n",
    "        for i in range(0,30):\n",
    "            for j in range(0,30):\n",
    "                norm[i][j] = np.linalg.norm(description_query[qkeypoint][i].cpu().numpy() - description_images[images_keypoint][j].cpu().numpy())\n",
    "        # Hungarian: one-to-one matching\n",
    "        indexes = hungarian.compute(np.copy(norm))\n",
    "        exponential = np.zeros(len(indexes))\n",
    "        for k in range(0,len(indexes)):\n",
    "            exponential[k] = np.exp(-(norm[indexes[k][0]][indexes[k][1]]))\n",
    "        one_to_one_similitude_matrix[qkeypoint][images_keypoint] = exponential.sum()\n",
    "\n",
    "\n",
    "print(one_to_one_similitude_matrix[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(one_to_one_similitude_matrix[34])\n",
    "torch.save(one_to_one_similitude_matrix, \"one_to_one_similitude_matrix.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 140)\n"
     ]
    }
   ],
   "source": [
    "one_to_one_similitude_matrix = torch.load(\"one_to_one_similitude_matrix.pt\")\n",
    "print(one_to_one_similitude_matrix.shape)\n",
    "#print(one_to_one_similitude_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.31519849e-02  8.14727766e-02 -1.29814923e+04 -5.47319828e+04\n",
      " -4.61840247e+05 -2.15849766e+05 -4.12740950e+05 -3.09157317e+05\n",
      " -2.63627611e+05 -1.10448697e+05 -2.19325986e+05 -1.27932807e+05\n",
      " -1.97779336e+05 -2.20126239e+05 -3.06763923e+05 -1.09685515e+05\n",
      " -3.14637464e+05 -1.25039273e+05 -5.61586204e+04 -1.78171715e+04\n",
      " -4.29157600e+05 -6.50259729e+05 -1.56877759e+05 -1.02180575e+05\n",
      " -3.12469240e+05 -4.45683336e+05 -3.39442944e+05 -1.13153726e+05\n",
      " -8.20462384e+04 -4.51594889e+05 -6.80824571e+05 -1.01101369e+05\n",
      " -3.06617738e+05 -2.21981214e+05 -3.05005754e+05 -1.10999735e+05\n",
      " -1.05443791e+06 -9.67001732e+04 -4.70503660e+05 -3.76705898e+05\n",
      " -6.52461152e+05 -4.00983397e+05 -3.85481612e+05 -2.74445567e+05\n",
      " -1.85617181e+05 -3.04204290e+05 -1.21385717e+06 -1.09663608e+05\n",
      " -1.59942637e+04 -3.06740967e+05 -4.87622068e+04 -1.18659851e+05\n",
      " -5.81084793e+04 -1.35222452e+05 -3.70033684e+05 -1.12598191e+05\n",
      " -3.14742595e+04 -1.86762322e+05 -8.83548165e+05 -3.72384274e+05\n",
      " -3.19216272e+05 -3.15307684e+05 -5.16049060e+04 -4.02362654e+05\n",
      " -3.88537134e+05 -5.41644695e+05 -4.83257979e+04 -1.75734740e+05\n",
      " -1.96967104e+05 -4.27035837e+05 -5.92569323e+05 -2.31485773e+05\n",
      " -2.78049840e+05 -3.60866037e+04 -2.17169528e+05 -3.28471575e+05\n",
      " -9.97469337e+04 -2.47288691e+05 -3.92414514e+05 -3.71551295e+05\n",
      " -2.83315044e+05 -1.75257993e+05 -3.15470343e+05 -3.23364096e+05\n",
      " -3.95079326e+05 -4.26056349e+05 -1.54359014e+05 -1.47062653e+05\n",
      " -1.77505908e+05 -3.65054120e+05 -2.55905456e+05 -2.37570448e+05\n",
      " -1.50811413e+05 -3.07363232e+05 -6.08965272e+05 -5.45267295e+05\n",
      " -4.54054993e+05 -3.55983395e+05 -5.05502118e+05 -2.29800880e+05\n",
      " -4.23461313e+05 -2.62289004e+05 -7.41784562e+04 -2.64409100e+05\n",
      " -2.82008750e+05 -6.71942635e+05 -1.52708758e+05 -2.62597235e+05\n",
      " -3.43709077e+05 -3.02683035e+05 -7.49856720e+05 -3.00116146e+05\n",
      " -7.34401492e+04 -2.27555327e+05 -1.51046857e+05 -2.08194893e+05\n",
      " -1.77926891e+05 -2.56339317e+05 -2.02093876e+05 -1.32447458e+05\n",
      " -6.86154298e+05 -5.42006373e+05 -1.40558442e+05 -3.87313380e+05\n",
      " -4.87179668e+04 -1.43250364e+05 -6.50780512e+04 -6.56370850e+05\n",
      " -5.89172937e+05 -1.94914641e+05 -2.99899510e+05 -5.12786274e+05\n",
      " -8.98911810e+04 -4.49534045e+05 -4.56224091e+04 -6.30215727e+04\n",
      " -9.17087791e+05 -1.44185182e+05 -2.96408120e+05 -5.78285361e+05]\n"
     ]
    }
   ],
   "source": [
    "many_to_many_similitude_matrix = np.zeros((35,140))\n",
    "threshold = 0.1\n",
    "for qkeypoint in range(0,35):\n",
    "    for images_keypoint in range(0,140):\n",
    "        dist = np.zeros(30)\n",
    "        for i in range(0,30):\n",
    "            tmp = np.zeros(30)\n",
    "            for j in range(k,30):\n",
    "                query = description_query[qkeypoint][i].cpu().numpy()\n",
    "                image = description_images[images_keypoint][j].cpu().numpy()\n",
    "                tmp[i] = np.sum(np.absolute(np.subtract(query,image)))\n",
    "            dist[i] = np.linalg.norm(tmp)\n",
    "        s = np.zeros(30)\n",
    "        x = np.zeros(30)\n",
    "        x_s = np.zeros(30)\n",
    "        for k in range (0,30):\n",
    "            s[k] = np.exp(-(dist[k]))\n",
    "        lamda = np.sqrt(1 / np.dot(np.transpose(s), s))\n",
    "        norm = np.linalg.norm(s,axis=0)\n",
    "        x = np.true_divide(s, norm)\n",
    "        x_s = np.copy(x)\n",
    "        x_s[x_s > threshold] = 1\n",
    "        x_s[x_s < threshold] = 0\n",
    "        a = np.dot(np.transpose(s), x_s)\n",
    "        b = lamda * (np.dot(np.transpose(x_s), x_s) - 1)\n",
    "        many_to_many_similitude_matrix[qkeypoint][images_keypoint]= a - b\n",
    "\n",
    "\n",
    "print(many_to_many_similitude_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.31519849e-02  8.14727766e-02 -1.29814923e+04 -5.47319828e+04\n",
      " -4.61840247e+05 -2.15849766e+05 -4.12740950e+05 -3.09157317e+05\n",
      " -2.63627611e+05 -1.10448697e+05 -2.19325986e+05 -1.27932807e+05\n",
      " -1.97779336e+05 -2.20126239e+05 -3.06763923e+05 -1.09685515e+05\n",
      " -3.14637464e+05 -1.25039273e+05 -5.61586204e+04 -1.78171715e+04\n",
      " -4.29157600e+05 -6.50259729e+05 -1.56877759e+05 -1.02180575e+05\n",
      " -3.12469240e+05 -4.45683336e+05 -3.39442944e+05 -1.13153726e+05\n",
      " -8.20462384e+04 -4.51594889e+05 -6.80824571e+05 -1.01101369e+05\n",
      " -3.06617738e+05 -2.21981214e+05 -3.05005754e+05 -1.10999735e+05\n",
      " -1.05443791e+06 -9.67001732e+04 -4.70503660e+05 -3.76705898e+05\n",
      " -6.52461152e+05 -4.00983397e+05 -3.85481612e+05 -2.74445567e+05\n",
      " -1.85617181e+05 -3.04204290e+05 -1.21385717e+06 -1.09663608e+05\n",
      " -1.59942637e+04 -3.06740967e+05 -4.87622068e+04 -1.18659851e+05\n",
      " -5.81084793e+04 -1.35222452e+05 -3.70033684e+05 -1.12598191e+05\n",
      " -3.14742595e+04 -1.86762322e+05 -8.83548165e+05 -3.72384274e+05\n",
      " -3.19216272e+05 -3.15307684e+05 -5.16049060e+04 -4.02362654e+05\n",
      " -3.88537134e+05 -5.41644695e+05 -4.83257979e+04 -1.75734740e+05\n",
      " -1.96967104e+05 -4.27035837e+05 -5.92569323e+05 -2.31485773e+05\n",
      " -2.78049840e+05 -3.60866037e+04 -2.17169528e+05 -3.28471575e+05\n",
      " -9.97469337e+04 -2.47288691e+05 -3.92414514e+05 -3.71551295e+05\n",
      " -2.83315044e+05 -1.75257993e+05 -3.15470343e+05 -3.23364096e+05\n",
      " -3.95079326e+05 -4.26056349e+05 -1.54359014e+05 -1.47062653e+05\n",
      " -1.77505908e+05 -3.65054120e+05 -2.55905456e+05 -2.37570448e+05\n",
      " -1.50811413e+05 -3.07363232e+05 -6.08965272e+05 -5.45267295e+05\n",
      " -4.54054993e+05 -3.55983395e+05 -5.05502118e+05 -2.29800880e+05\n",
      " -4.23461313e+05 -2.62289004e+05 -7.41784562e+04 -2.64409100e+05\n",
      " -2.82008750e+05 -6.71942635e+05 -1.52708758e+05 -2.62597235e+05\n",
      " -3.43709077e+05 -3.02683035e+05 -7.49856720e+05 -3.00116146e+05\n",
      " -7.34401492e+04 -2.27555327e+05 -1.51046857e+05 -2.08194893e+05\n",
      " -1.77926891e+05 -2.56339317e+05 -2.02093876e+05 -1.32447458e+05\n",
      " -6.86154298e+05 -5.42006373e+05 -1.40558442e+05 -3.87313380e+05\n",
      " -4.87179668e+04 -1.43250364e+05 -6.50780512e+04 -6.56370850e+05\n",
      " -5.89172937e+05 -1.94914641e+05 -2.99899510e+05 -5.12786274e+05\n",
      " -8.98911810e+04 -4.49534045e+05 -4.56224091e+04 -6.30215727e+04\n",
      " -9.17087791e+05 -1.44185182e+05 -2.96408120e+05 -5.78285361e+05]\n"
     ]
    }
   ],
   "source": [
    "torch.save(many_to_many_similitude_matrix, \"many_to_many_similitude_matrix.pt\")\n",
    "print(many_to_many_similitude_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth =  np.subtract(np.array([[1, 1], [1, 2], [1, 3], [1, 4], [2, 5], [2, 6], [2, 7], [2, 8], [3, 9], \n",
    "[3, 10], [3, 11], [3, 12], [4, 13], [4, 14], [4, 15], [4, 16], [5, 17], [5, 18], [5, 19], [5, 20], \n",
    "[6, 21], [6, 22], [6, 23], [6, 24], [7, 25], [7, 26], [7, 27], [7, 28], [8, 29], [8, 30], [8, 31], \n",
    "[8, 32], [9, 33], [9, 34], [9, 35], [9, 36], [10, 37], [10, 38], [10, 39], [10, 40], [11, 41], \n",
    "[11, 42], [11, 43], [11, 44], [12, 45], [12, 46], [12, 47], [12, 48], [13, 49], [13, 50], [13, 51], \n",
    "[13, 52], [14, 53], [14, 54], [14, 55], [14, 56], [15, 57], [15, 58], [15, 59], [15, 60], [16, 61], \n",
    "[16, 62], [16, 63], [16, 64], [17, 65], [17, 66], [17, 67], [17, 68], [18, 69], [18, 70], [18, 71], \n",
    "[18, 72], [19, 73], [19, 74], [19, 75], [19, 76], [20, 77], [20, 78], [20, 79], [20, 80], [21, 81], \n",
    "[21, 82], [21, 83], [21, 84], [22, 85], [22, 86], [22, 87], [22, 88], [23, 89], [23, 90], [23, 91], \n",
    "[23, 92], [24, 93], [24, 94], [24, 95], [24, 96], [25, 97], [25, 98], [25, 99], [25, 100], [26, 101], \n",
    "[26, 102], [26, 103], [26, 104], [27, 105], [27, 106], [27, 107], [27, 108], [28, 109], [28, 110], \n",
    "[28, 111], [28, 112], [29, 113], [29, 114], [29, 115], [29, 116], [30, 117], [30, 118], [30, 119], \n",
    "[30, 120], [31, 121], [31, 122], [31, 123], [31, 124], [32, 125], [32, 126], [32, 127], [32, 128], \n",
    "[33, 129], [33, 130], [33, 131], [33, 132], [34, 133], [34, 134], [34, 135], [34, 136], [35, 137], \n",
    "[35, 138], [35, 139], [35, 140]]),1)\n",
    "#print(ground_truth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function which can be used to determine precision and recall of the matching with as inputs: similitude_matrix, ground_truth, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42857142857142855, 0.10714285714285714]\n",
      "[0.7571428571428571, 0.7571428571428571]\n"
     ]
    }
   ],
   "source": [
    "# Determine precision and recall\n",
    "def precision_and_recall_matching(similitude_matrix, ground_truth, k):\n",
    "    similitude_matrix = similitude_matrix.copy()\n",
    "    precision_matrix = np.zeros((4,similitude_matrix.shape[0]))\n",
    "    recall_matrix = np.zeros((4,similitude_matrix.shape[0]))\n",
    "    highest_matching_points = np.zeros((4,similitude_matrix.shape[0]))\n",
    "    \n",
    "   # for i in range(35):\n",
    "    #    highest_matching_points[i] = Nmaxelements(k, similitude_matrix)\n",
    "    \n",
    "    for i in range(0,similitude_matrix.shape[0]):\n",
    "        # sort the highest matching points in a list k-long\n",
    "        highest_matching_points = (-similitude_matrix[i]).argsort()[:k]\n",
    "        #print((-similitude_matrix[i]).argsort())\n",
    "        true_positives = 0\n",
    "        for j in range(0,k):\n",
    "            index = 4 * i + j\n",
    "            if ground_truth[index][1] in highest_matching_points:\n",
    "           # if ground_truth[index][1] in highest_matching_points:\n",
    "                true_positives += 1\n",
    "        precision_matrix[k-1][i] = true_positives / k\n",
    "        recall_matrix[k-1][i] = true_positives / 4\n",
    "    \n",
    "    average_precision = np.sum(precision_matrix[k-1])/similitude_matrix.shape[0]\n",
    "    average_recall = np.sum(recall_matrix[k-1])/similitude_matrix.shape[0]\n",
    "    return [float(average_precision), float(average_recall)]\n",
    "\n",
    "#one_to_one_similitude_matrix = one_to_one_similitude_matrix.numpy()\n",
    "print(precision_and_recall_matching(one_to_one_similitude_matrix, ground_truth, 1))\n",
    "print(precision_and_recall_matching(one_to_one_similitude_matrix, ground_truth, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Matching One to One:\n",
      "[[1.         2.         3.         4.        ]\n",
      " [0.42857143 0.48571429 0.66666667 0.75714286]]\n",
      "Recall Matching One to One:\n",
      "[[1.         2.         3.         4.        ]\n",
      " [0.10714286 0.24285714 0.5        0.75714286]]\n",
      "----------------------------------------------------\n",
      "Precision Matching Many to Many:\n",
      "[[1.         2.         3.         4.        ]\n",
      " [0.25714286 0.28571429 0.42857143 0.47142857]]\n",
      "Recall Matching Many to Many:\n",
      "[[1.         2.         3.         4.        ]\n",
      " [0.06428571 0.14285714 0.32142857 0.47142857]]\n"
     ]
    }
   ],
   "source": [
    "# Load similitude matrixes: one-to-one & many-to-many\n",
    "one_to_one_similitude_matrix = torch.load(\"results/one_to_one_similitude_matrix.pt\")\n",
    "one_to_one_similitude_matrix = one_to_one_similitude_matrix.numpy()\n",
    "many_to_many_similitude_matrix = torch.load(\"results/many_to_many_similitude_matrix.pt\")\n",
    "many_to_many_similitude_matrix = many_to_many_similitude_matrix.numpy()\n",
    "\n",
    "\n",
    "precision_matching_one_to_one = np.zeros((2,4))\n",
    "precision_matching_many_to_many = np.zeros((2,4))\n",
    "recall_matching_one_to_one = np.zeros((2,4))\n",
    "recall_matching_many_to_many = np.zeros((2,4))\n",
    "\n",
    "for k in range(1,5):\n",
    "    precision_matching_one_to_one[1][k-1] = precision_and_recall_matching(one_to_one_similitude_matrix, \n",
    "                                                                        ground_truth, k)[0]\n",
    "    precision_matching_many_to_many[1][k-1] = precision_and_recall_matching(many_to_many_similitude_matrix, \n",
    "                                                                        ground_truth, k)[0]\n",
    "    precision_matching_one_to_one[0][k-1] = k\n",
    "    precision_matching_many_to_many[0][k-1] = k\n",
    "    \n",
    "    recall_matching_one_to_one[1][k-1] = precision_and_recall_matching(one_to_one_similitude_matrix, \n",
    "                                                                       ground_truth, k)[1]\n",
    "    recall_matching_many_to_many[1][k-1] = precision_and_recall_matching(many_to_many_similitude_matrix, \n",
    "                                                                         ground_truth, k)[1]\n",
    "    recall_matching_one_to_one[0][k-1] = k\n",
    "    recall_matching_many_to_many[0][k-1] = k\n",
    "    \n",
    "    \n",
    "print(\"Precision Matching One to One:\")\n",
    "print(precision_matching_one_to_one)\n",
    "print(\"Recall Matching One to One:\")\n",
    "print(recall_matching_one_to_one)\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"Precision Matching Many to Many:\")\n",
    "print(precision_matching_many_to_many)\n",
    "print(\"Recall Matching Many to Many:\")\n",
    "print(recall_matching_many_to_many)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
